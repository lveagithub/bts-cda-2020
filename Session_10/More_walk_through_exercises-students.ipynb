{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Source\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import export_graphviz\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn.datasets \n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(\n",
    "    y_test:pd.core.frame.DataFrame,\n",
    "    pred_nb:np.array,\n",
    "    target_labels:list,\n",
    "):\n",
    "    mat = confusion_matrix(y_test, pred_nb)\n",
    "    sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "                xticklabels=target_labels, yticklabels=target_labels)\n",
    "    plt.xlabel('true label')\n",
    "    plt.ylabel('predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Guassian Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Activity Recognition, or HAR for short\n",
    "Refers to the problem of predicting what a person is doing based on a trace of their movement using sensors.\n",
    "Movements are often normal indoor activities such as standing, sitting, jumping, and going up stairs.\n",
    "Sensors are often located on the subject, such as a smartphone or vest, and often record accelerometer data in three dimensions (x, y, z).\n",
    "Human Activity Recognition (HAR) aims to identify the actions carried out by a person given a set of observations of him/herself and the surrounding environment. Recognition can be accomplished by exploiting the information retrieved from various sources such as environmental or body-worn sensors.\n",
    "\n",
    "â€” A Public Domain Dataset for Human Activity Recognition Using Smartphones, 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_dataset = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip'\n",
    "\n",
    "zipresp = urlopen(url_dataset)\n",
    "tempzip = open('tempfile.zip', 'wb')\n",
    "tempzip.write(zipresp.read())\n",
    "tempzip.close()\n",
    "zf = ZipFile('tempfile.zip')\n",
    "zf.extractall(path = 'dataset/')\n",
    "zf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load columns names\n",
    "fread = open('dataset/UCI HAR Dataset/features.txt', 'r')\n",
    "columns = fread.readlines()\n",
    "columns = [col\\\n",
    "           .replace('\\n', '')\\\n",
    "           .replace('(', '')\\\n",
    "           .replace(')', '')\\\n",
    "           .replace('-', '_')\\\n",
    "           .replace(',', '_')\\\n",
    "           .lower().split()[1] for col in columns]\n",
    "fread.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fread = open('dataset/UCI HAR Dataset/activity_labels.txt', 'r')\n",
    "target_labels = fread.readlines()\n",
    "target_labels = [col\\\n",
    "                 .replace('\\n', '')\\\n",
    "                 .replace('(', '')\\\n",
    "                 .replace(')', '')\\\n",
    "                 .replace('-', '_')\\\n",
    "                 .replace(',', '_').lower().split()[1] for col in target_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "    dataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
    "    return dataframe\n",
    "\n",
    "X = load_file('dataset/UCI HAR Dataset/train/X_train.txt')\n",
    "X.columns = columns\n",
    "y = load_file('dataset/UCI HAR Dataset/train/y_train.txt')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a diverse set of nonlinear and ensemble machine learning algorithms\n",
    "\n",
    "Nonlinear Algorithms:\n",
    "\n",
    "    Classification and Regression Tree\n",
    "    Support Vector Machine\n",
    "    Naive Bayes\n",
    "\n",
    "Ensemble Algorithms:\n",
    "\n",
    "    Bagged Decision Trees\n",
    "    Random Forest\n",
    "    Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a predictive model using Gaussian Naive Bayes\n",
    "- No need to to parameter tunning; use default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run classification report and plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune your Gaussian Naive Bayes\n",
    "- var_smoothing: Portion of the largest variance of all features that is added to variances for calculation stability.\n",
    "\n",
    "[Sklearn Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Explain in your own words what do you think var_smoothing is doing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer: \n",
    "Naive Bayes uses the a Gaussian distribution which attributes more weights to the samples closer to the distribution mean.\n",
    "This might or might not be appropriate depending if what you want to predict follows a normal distribution.\n",
    "\n",
    "This parameter (var_smoothing) allows to add distribution variance (default value is taken from the training dataset) which essencially widens/smooths the curve by taking into account samples that are further away from the distribution mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, tune your model and see if you can improve the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run classification report and see improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a predictive model using SVM\n",
    "- try a linear and non-linear kernel\n",
    "- no need to normalize the data because is already prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a predictive model using Decision Trees\n",
    "- Parameter tuning for max depth from 2 to 4\n",
    "- Min sample nodes to split from 20% to 40%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BaggingClassifier\n",
    "- Run 200 Decision Trees with 50 samples each and with bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (?) Same performance as before... what do you think is happening with this data? Try decreasing max_samples... what do you think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
